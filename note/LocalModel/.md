Khuyáº¿n nghá»‹ cuá»‘i cÃ¹ng

1. Cho Embedding:
- Lá»±a chá»n tá»‘t nháº¥t: Vietnamese_Embedding cá»§a AITeamVN â€“ Ä‘Æ°á»£c tinh chá»‰nh Ä‘áº·c biá»‡t cho tiáº¿ng Viá»‡t, hiá»‡u suáº¥t vÆ°á»£t trá»™i so vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯

2. Cho LLM 3B-7B:
- Cho thiáº¿t bá»‹ cáº¥u hÃ¬nh tháº¥p/trung bÃ¬nh: Llama-3.2-3B-Instruct-Frog â€“ tá»‘i Æ°u cho RAG tiáº¿ng Viá»‡t vÃ  cÃ³ thá»ƒ cháº¡y trÃªn nhiá»u loáº¡i thiáº¿t bá»‹
- Cho hiá»‡u suáº¥t cao hÆ¡n: PhoGPT-7.5B hoáº·c Vi-Qwen2-7B-RAG náº¿u báº¡n cÃ³ Ä‘á»§ tÃ i nguyÃªn pháº§n cá»©ng

---
https://www.genspark.ai/agents?id=24eb6e6a-3d63-4eec-8cc8-198e59552223
---

# KHUYáº¾N NGHá»Š CUá»I CÃ™NG CHO Há»† THá»NG RAG TIáº¾NG VIá»†T

## Cáº¤U HÃŒNH MÃY CHO EMBEDDING VÃ€ LLM

### 1. Cáº¤U HÃŒNH Tá»I THIá»‚U Äá»‚ CHáº Y Cáº¢ EMBEDDING VÃ€ LLAMA-3.2-3B-INSTRUCT

```
ğŸ”¹ CPU: Intel Core i5 tháº¿ há»‡ 10 trá»Ÿ lÃªn / AMD Ryzen 5 3000 series trá»Ÿ lÃªn
ğŸ”¹ RAM: 16GB
ğŸ”¹ GPU: NVIDIA GPU vá»›i Ã­t nháº¥t 6GB VRAM (nhÆ° GTX 1660 Ti trá»Ÿ lÃªn)
ğŸ”¹ á»” cá»©ng: SSD 20GB trá»‘ng
ğŸ”¹ Há»‡ Ä‘iá»u hÃ nh: Windows 10/11 64-bit, Ubuntu 20.04 trá»Ÿ lÃªn
```

### 2. Cáº¤U HÃŒNH Äá»€ XUáº¤T CHO HIá»†U SUáº¤T Tá»T

```
ğŸ”¹ CPU: Intel Core i7/i9 tháº¿ há»‡ 11 trá»Ÿ lÃªn / AMD Ryzen 7/9 5000 series trá»Ÿ lÃªn
ğŸ”¹ RAM: 32GB
ğŸ”¹ GPU: NVIDIA RTX 3060 (8GB VRAM) trá»Ÿ lÃªn
ğŸ”¹ á»” cá»©ng: NVMe SSD vá»›i Ã­t nháº¥t 50GB trá»‘ng
ğŸ”¹ Há»‡ Ä‘iá»u hÃ nh: Windows 10/11 64-bit, Ubuntu 20.04 trá»Ÿ lÃªn
```

### 3. YÃŠU Cáº¦U RIÃŠNG CHO Tá»ªNG THÃ€NH PHáº¦N

| MÃ´ hÃ¬nh | CPU | RAM | GPU | á»” cá»©ng | Ghi chÃº |
|---------|-----|-----|-----|---------|---------|
| **AITeamVN/Vietnamese_Embedding** | Core i5/Ryzen 5 | 4-8GB | KhÃ´ng báº¯t buá»™c | 500MB | CÃ³ thá»ƒ cháº¡y CPU-only |
| **Llama-3.2-3B-Instruct-Frog** | Core i7/Ryzen 7 | 8-16GB | 6GB VRAM | 10GB | Tá»‘i Æ°u hiá»‡u suáº¥t vá»›i GPU |

## Lá»¢I ÃCH KHI Sá»¬ Dá»¤NG CÃC MÃ” HÃŒNH Äá»€ XUáº¤T

### AITeamVN/Vietnamese_Embedding

- **Hiá»‡u suáº¥t**: VÆ°á»£t trá»™i trong viá»‡c hiá»ƒu ngá»¯ nghÄ©a tiáº¿ng Viá»‡t
- **TÃ i nguyÃªn**: Nháº¹ hÆ¡n cÃ¡c mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ (768 chiá»u thay vÃ¬ 1024)
- **Äá»‹nh lÆ°á»£ng**: ÄÆ°á»£c tinh chá»‰nh cho má»¥c Ä‘Ã­ch RAG tiáº¿ng Viá»‡t

### Llama-3.2-3B-Instruct-Frog

- **CÃ¢n báº±ng**: Äáº¡t cÃ¢n báº±ng tá»‘t giá»¯a kÃ­ch thÆ°á»›c vÃ  hiá»‡u suáº¥t
- **Ngá»¯ cáº£nh**: CÃ³ thá»ƒ xá»­ lÃ½ ngá»¯ cáº£nh dÃ i cho RAG
- **Tá»‘i Æ°u hÃ³a**: PhiÃªn báº£n GGUF Ä‘Æ°á»£c tá»‘i Æ°u cho CPU vÃ  GPU yÃªu cáº§u tháº¥p

## HÆ¯á»šNG DáºªN TRIá»‚N KHAI Tá»I Æ¯U

### 1. Thiáº¿t láº­p mÃ´i trÆ°á»ng

```bash
# Táº¡o mÃ´i trÆ°á»ng conda
conda create -n rag_env python=3.10
conda activate rag_env

# CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t
pip install sentence-transformers torch langchain qdrant_client llama-cpp-python uvicorn fastapi
```

### 2. Táº£i vÃ  cÃ i Ä‘áº·t mÃ´ hÃ¬nh embedding

```python
from sentence_transformers import SentenceTransformer

# Táº¡o thÆ° má»¥c lÆ°u mÃ´ hÃ¬nh
import os
os.makedirs("models/embedding", exist_ok=True)

# Táº£i mÃ´ hÃ¬nh Vietnamese_Embedding
model = SentenceTransformer("AITeamVN/Vietnamese_Embedding", cache_folder="models/embedding")

# Kiá»ƒm tra
test_embedding = model.encode("Kiá»ƒm tra mÃ´ hÃ¬nh embedding tiáº¿ng Viá»‡t")
print(f"KÃ­ch thÆ°á»›c vector: {len(test_embedding)}")
```

### 3. CÃ i Ä‘áº·t mÃ´ hÃ¬nh LLM 3B

```bash
# Táº£i mÃ´ hÃ¬nh GGUF Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u
mkdir -p models/llm
wget -O models/llm/llama-3.2-3B-instruct-frog-Q5_K_M.gguf https://huggingface.co/TheBloke/Llama-3.2-3B-Instruct-GGUF/resolve/main/llama-3.2-3b-instruct.Q5_K_M.gguf
```

### 4. Thiáº¿t láº­p dá»‹ch vá»¥ embedding

```python
# embedding_service.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Union
from sentence_transformers import SentenceTransformer
import uvicorn

app = FastAPI()
model_path = "AITeamVN/Vietnamese_Embedding"
cache_dir = "models/embedding"

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
model = SentenceTransformer(model_path, cache_folder=cache_dir)

class EmbeddingRequest(BaseModel):
    texts: Union[str, List[str]]

@app.post("/embed")
def create_embedding(request: EmbeddingRequest):
    try:
        texts = request.texts if isinstance(request.texts, list) else [request.texts]
        embeddings = model.encode(texts)
        return {"embeddings": embeddings.tolist()}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run("embedding_service:app", host="0.0.0.0", port=8001)
```

### 5. Thiáº¿t láº­p dá»‹ch vá»¥ LLM

```python
# llm_service.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import uvicorn
from llama_cpp import Llama

app = FastAPI()
model_path = "models/llm/llama-3.2-3B-instruct-frog-Q5_K_M.gguf"

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh vá»›i tá»‘i Æ°u hÃ³a cho GPU
model = Llama(
    model_path=model_path,
    n_ctx=4096,  # Ngá»¯ cáº£nh dÃ i cho RAG
    n_gpu_layers=-1,  # Tá»± Ä‘á»™ng dÃ¹ng háº¿t GPU layers
    n_threads=4  # Äiá»u chá»‰nh theo sá»‘ lÃµi CPU
)

class GenerationRequest(BaseModel):
    prompt: str
    max_tokens: int = 1024
    temperature: float = 0.7

@app.post("/generate")
def generate_text(request: GenerationRequest):
    try:
        output = model.create_completion(
            request.prompt, 
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        return {"generated_text": output["choices"][0]["text"]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run("llm_service:app", host="0.0.0.0", port=8002)
```

### 6. Docker Compose Ä‘á»ƒ Äiá»u phá»‘i cÃ¡c Dá»‹ch vá»¥

```yaml
version: '3'

services:
  embedding:
    build: 
      context: ./embedding
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    volumes:
      - ./models/embedding:/app/models/embedding
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  llm:
    build:
      context: ./llm
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    volumes:
      - ./models/llm:/app/models/llm
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 12G
    environment:
      - CUDA_VISIBLE_DEVICES=0

  rag-backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    depends_on:
      - embedding
      - llm
    environment:
      - EMBEDDING_URL=http://embedding:8001
      - LLM_URL=http://llm:8002

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - ./data/qdrant:/qdrant/storage
```

## TIP Bá»” SUNG

### 1. Tá»‘i Æ°u hÃ³a vá»›i GPU háº¡n cháº¿ (4-6GB VRAM)

```python
# Sá»­ dá»¥ng bá»™ nhá»› ná»­a chÃ­nh xÃ¡c
model = Llama(
    model_path=model_path,
    n_gpu_layers=-1,
    n_threads=4,
    n_batch=512,  # Giáº£m kÃ­ch thÆ°á»›c batch
    f16_kv=True,  # Sá»­ dá»¥ng bá»™ nhá»› ná»­a chÃ­nh xÃ¡c cho KV cache
    use_mlock=False
)
```

### 2. LÆ°u trá»¯ mÃ´ hÃ¬nh trÃªn á»• Ä‘Ä©a cháº­m (HDD)

CÃ¡ch tá»• chá»©c dá»¯ liá»‡u Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t:
- Táº¡o bá»™ nhá»› Ä‘á»‡m trÃªn RAM hoáº·c SSD cho pháº§n Ä‘Æ°á»£c truy cáº­p thÆ°á»ng xuyÃªn
- LÆ°u trá»¯ toÃ n bá»™ mÃ´ hÃ¬nh trÃªn HDD
- Khi khá»Ÿi Ä‘á»™ng, sao chÃ©p vÃ o bá»™ nhá»› Ä‘á»‡m

```python
import shutil
import os

# ÄÆ°á»ng dáº«n mÃ´ hÃ¬nh trÃªn HDD
hdd_model_path = "/mnt/hdd/models/vietnamese_embedding"

# ÄÆ°á»ng dáº«n bá»™ nhá»› Ä‘á»‡m trÃªn SSD
ssd_cache_path = "/tmp/models/embedding"

# Sao chÃ©p file quan trá»ng sang SSD khi khá»Ÿi Ä‘á»™ng
os.makedirs(ssd_cache_path, exist_ok=True)
for file in ["config.json", "modules.json", "1_Pooling/config.json", "pytorch_model.bin"]:
    src = os.path.join(hdd_model_path, file)
    dst = os.path.join(ssd_cache_path, file)
    if os.path.exists(src):
        os.makedirs(os.path.dirname(dst), exist_ok=True)
        shutil.copy(src, dst)

# Sá»­ dá»¥ng mÃ´ hÃ¬nh tá»« bá»™ nhá»› Ä‘á»‡m
model = SentenceTransformer(ssd_cache_path)
```

## LÆ¯U Ã QUAN TRá»ŒNG

1. **Quáº£n lÃ½ bá»™ nhá»› GPU**: Náº¿u cháº¡y cáº£ LLM vÃ  mÃ´ hÃ¬nh embedding trÃªn cÃ¹ng GPU, hÃ£y Ä‘áº·t thÃ´ng sá»‘ `max_memory` Ä‘á»ƒ kiá»ƒm soÃ¡t bá»™ nhá»› VRAM cho tá»«ng mÃ´ hÃ¬nh

2. **Bá»™ nhá»› Ä‘á»‡m embeddings**: CÃ¢n nháº¯c lÆ°u trá»¯ vÃ  cache embeddings cho vÄƒn báº£n thÆ°á»ng xuyÃªn sá»­ dá»¥ng Ä‘á»ƒ trÃ¡nh tÃ­nh toÃ¡n láº¡i

3. **Láº­p lá»‹ch batch**: Vá»›i nguá»“n lá»±c háº¡n cháº¿, xá»­ lÃ½ nhiá»u yÃªu cáº§u nhá» cÃ¹ng lÃºc hiá»‡u quáº£ hÆ¡n má»™t yÃªu cáº§u lá»›n

4. **Äá»‹nh lÆ°á»£ng mÃ´ hÃ¬nh**: GGUF Q5_K_M Ä‘áº¡t Ä‘Æ°á»£c sá»± cÃ¢n báº±ng tá»‘t giá»¯a cháº¥t lÆ°á»£ng vÃ  dung lÆ°á»£ng bá»™ nhá»›. Náº¿u RAM/VRAM háº¡n cháº¿, hÃ£y dÃ¹ng Q4_K_M

Vá»›i cáº¥u hÃ¬nh mÃ¡y Ä‘Æ°á»£c Ä‘á» xuáº¥t, báº¡n cÃ³ thá»ƒ cháº¡y cáº£ Vietnamese_Embedding vÃ  mÃ´ hÃ¬nh LLM 3B má»™t cÃ¡ch hiá»‡u quáº£, Ä‘áº£m báº£o pháº£n há»“i nhanh cho há»‡ thá»‘ng RAG cá»§a mÃ¬nh.