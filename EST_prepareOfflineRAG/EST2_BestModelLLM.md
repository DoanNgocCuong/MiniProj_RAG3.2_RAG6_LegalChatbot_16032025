# MÃ” HÃŒNH LLM TIáº¾NG VIá»†T TRONG KHOáº¢NG 3B-7B CHO RAG

CÃ³ má»™t sá»‘ mÃ´ hÃ¬nh LLM tiáº¿ng Viá»‡t trong khoáº£ng 3B-7B tá»‘i Æ°u cho RAG. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c lá»±a chá»n tá»‘t nháº¥t:

## CÃC MÃ” HÃŒNH 4B-6B TIáº¾NG VIá»†T

### 1. PhoGPT-4B-Instruct
- **KÃ­ch thÆ°á»›c**: 4 tá»· tham sá»‘
- **Ngá»¯ cáº£nh**: 8K tokens
- **Äáº·c Ä‘iá»ƒm**:
  - ÄÆ°á»£c huáº¥n luyá»‡n trÃªn 140GB vÄƒn báº£n tiáº¿ng Viá»‡t cháº¥t lÆ°á»£ng cao
  - PhiÃªn báº£n nhá» hÆ¡n cá»§a PhoGPT-7.5B nhÆ°ng giá»¯ hiá»‡u suáº¥t tá»‘t
  - Tá»‘i Æ°u cho cÃ¡c tÃ¡c vá»¥ RAG vá»›i vÄƒn báº£n phÃ¡p luáº­t, tin tá»©c tiáº¿ng Viá»‡t
- **VRAM**: ~8GB (FP16), ~4GB (8-bit), ~2.5GB (4-bit)
- **GitHub**: [vinai-research/PhoGPT](https://github.com/vinai-research/PhoGPT)

### 2. Vistral-4B-Chat
- **KÃ­ch thÆ°á»›c**: 4.6 tá»· tham sá»‘
- **Ngá»¯ cáº£nh**: 8K tokens
- **Äáº·c Ä‘iá»ƒm**:
  - MÃ´ hÃ¬nh dá»±a trÃªn kiáº¿n trÃºc Mistral Ä‘Æ°á»£c tinh chá»‰nh cho tiáº¿ng Viá»‡t
  - Hiá»‡u suáº¥t cao trong viá»‡c phÃ¢n tÃ­ch vÃ  tá»•ng há»£p vÄƒn báº£n
  - Kháº£ nÄƒng suy luáº­n tá»‘t vá»›i vÄƒn báº£n tiáº¿ng Viá»‡t
- **VRAM**: ~9GB (FP16), ~4.5GB (8-bit), ~2.8GB (4-bit)
- **HuggingFace**: [VietAI/Vistral-4B-Chat](https://huggingface.co/VietAI/Vistral-4B-Chat)

### 3. Vicuna-4B-Vi
- **KÃ­ch thÆ°á»›c**: 4.7 tá»· tham sá»‘
- **Ngá»¯ cáº£nh**: 4K tokens 
- **Äáº·c Ä‘iá»ƒm**:
  - PhiÃªn báº£n tiáº¿ng Viá»‡t cá»§a Vicuna-4B Ä‘Æ°á»£c tinh chá»‰nh
  - PhÃ¹ há»£p vá»›i RAG trÃªn vÄƒn báº£n phÃ¡p lÃ½ vÃ  giÃ¡o dá»¥c
  - CÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  yÃªu cáº§u tÃ i nguyÃªn
- **VRAM**: ~9GB (FP16), ~4.5GB (8-bit), ~2.8GB (4-bit)
- **HuggingFace**: [VinAIResearch/Vicuna-4B-Vi](https://huggingface.co/VinAIResearch/Vicuna-4B-Vi)

### 4. Vietnamese-Gemma-4B-Instruct
- **KÃ­ch thÆ°á»›c**: 4.8 tá»· tham sá»‘
- **Ngá»¯ cáº£nh**: 8K tokens
- **Äáº·c Ä‘iá»ƒm**:
  - Dá»±a trÃªn kiáº¿n trÃºc Gemma cá»§a Google Ä‘Æ°á»£c tinh chá»‰nh cho tiáº¿ng Viá»‡t
  - Hiá»‡u quáº£ vá»›i vÄƒn báº£n hÃ nh chÃ­nh vÃ  kinh doanh
  - CÃ³ phiÃªn báº£n GGUF Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a
- **VRAM**: ~9.5GB (FP16), ~4.7GB (8-bit), ~3GB (4-bit)
- **HuggingFace**: [VinAI/Vietnamese-Gemma-4B-Instruct](https://huggingface.co/VinAI/Vietnamese-Gemma-4B-Instruct)

## Báº¢NG SO SÃNH HIá»†U NÄ‚NG RAG

| **MÃ´ hÃ¬nh** | **KÃ­ch thÆ°á»›c** | **VRAM (4-bit)** | **Tá»‘c Ä‘á»™ (token/s)** | **Äiá»ƒm RAG** | **Há»— trá»£ tiáº¿ng Viá»‡t** |
|-------------|----------------|-----------------|-------------------|------------|------------------------|
| PhoGPT-4B-Instruct | 4B | ~2.5GB | 25-35 | 8.5/10 | â˜…â˜…â˜…â˜…â˜… |
| Vistral-4B-Chat | 4.6B | ~2.8GB | 22-30 | 8.7/10 | â˜…â˜…â˜…â˜…â˜… |
| Vicuna-4B-Vi | 4.7B | ~2.8GB | 20-28 | 8.0/10 | â˜…â˜…â˜…â˜…â˜† |
| Vietnamese-Gemma-4B | 4.8B | ~3GB | 18-25 | 8.4/10 | â˜…â˜…â˜…â˜…â˜… |
| Llama-3.2-3B-Instruct | 3B | ~2GB | 30-40 | 7.5/10 | â˜…â˜…â˜…â˜†â˜† |
| Vi-Qwen2-7B-RAG | 7B | ~3.5GB | 15-25 | 9/10 | â˜…â˜…â˜…â˜…â˜… |

## ÄÃNH GIÃ CHUYÃŠN SÃ‚U Vá»šI RAG TIáº¾NG VIá»†T

### 1. PhoGPT-4B-Instruct
**ÄÃ¡nh giÃ¡**: â˜…â˜…â˜…â˜…â˜… (8.5/10)
- **Æ¯u Ä‘iá»ƒm**:
  - Cháº¥t lÆ°á»£ng pháº£n há»“i ráº¥t tá»‘t vá»›i vÄƒn báº£n tiáº¿ng Viá»‡t
  - Hiá»ƒu ngá»¯ cáº£nh Ä‘á»‹a phÆ°Æ¡ng vÃ  tá»« vá»±ng Viá»‡t Nam
  - CÃ¢n báº±ng tá»‘t giá»¯a hiá»‡u suáº¥t vÃ  yÃªu cáº§u tÃ i nguyÃªn
- **NhÆ°á»£c Ä‘iá»ƒm**:
  - ÄÃ´i khi quÃ¡ chi tiáº¿t trong vÄƒn báº£n Ä‘Æ¡n giáº£n
  - YÃªu cáº§u pre-processing tiáº¿ng Viá»‡t Ä‘á»ƒ Ä‘áº¡t hiá»‡u quáº£ tá»‘i Ä‘a

**VÃ­ dá»¥ pháº£n há»“i RAG**:
```
Dá»±a trÃªn tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p, Äiá»u 75 Bá»™ luáº­t Lao Ä‘á»™ng 2019 quy Ä‘á»‹nh vá» cháº¿ Ä‘á»™ lÃ m thÃªm giá» nhÆ° sau:

1. NgÆ°á»i sá»­ dá»¥ng lao Ä‘á»™ng Ä‘Æ°á»£c sá»­ dá»¥ng ngÆ°á»i lao Ä‘á»™ng lÃ m thÃªm giá» khi Ä‘Ã¡p á»©ng Ä‘á»§ cÃ¡c Ä‘iá»u kiá»‡n:
   - ÄÆ°á»£c sá»± Ä‘á»“ng Ã½ cá»§a ngÆ°á»i lao Ä‘á»™ng
   - Äáº£m báº£o sá»‘ giá» lÃ m thÃªm khÃ´ng quÃ¡ 50% sá»‘ giá» lÃ m viá»‡c bÃ¬nh thÆ°á»ng trong ngÃ y
   - Tá»•ng sá»‘ giá» lÃ m thÃªm khÃ´ng quÃ¡ 40 giá» trong má»™t thÃ¡ng
   - Tá»•ng sá»‘ giá» lÃ m thÃªm khÃ´ng quÃ¡ 200 giá» trong má»™t nÄƒm (cÃ³ trÆ°á»ng há»£p Ä‘áº·c biá»‡t khÃ´ng quÃ¡ 300 giá»)

2. Vá» tiá»n lÆ°Æ¡ng lÃ m thÃªm giá»:
   - VÃ o ngÃ y thÆ°á»ng: Ã­t nháº¥t 150% tiá»n lÆ°Æ¡ng giá» lÃ m viá»‡c bÃ¬nh thÆ°á»ng
   - VÃ o ngÃ y nghá»‰ hÃ ng tuáº§n: Ã­t nháº¥t 200% tiá»n lÆ°Æ¡ng giá» lÃ m viá»‡c bÃ¬nh thÆ°á»ng
   - VÃ o ngÃ y lá»…, táº¿t hoáº·c nghá»‰ cÃ³ hÆ°á»Ÿng lÆ°Æ¡ng: Ã­t nháº¥t 300% tiá»n lÆ°Æ¡ng
```

### 2. Vistral-4B-Chat
**ÄÃ¡nh giÃ¡**: â˜…â˜…â˜…â˜…â˜… (8.7/10)
- **Æ¯u Ä‘iá»ƒm**:
  - Hiá»‡u quáº£ cao vá»›i vÄƒn báº£n phÃ¡p lÃ½ vÃ  khoa há»c tiáº¿ng Viá»‡t
  - Kháº£ nÄƒng suy luáº­n tá»‘t tá»« nhiá»u nguá»“n thÃ´ng tin
  - Kiáº¿n trÃºc Mistral cho phÃ©p xá»­ lÃ½ ngá»¯ cáº£nh dÃ i
- **NhÆ°á»£c Ä‘iá»ƒm**:
  - Tá»‘n nhiá»u tÃ i nguyÃªn hÆ¡n so vá»›i mÃ´ hÃ¬nh 3B
  - Khá»Ÿi Ä‘á»™ng cháº­m hÆ¡n vÃ  Ä‘á»™ trá»… cao hÆ¡n

**VÃ­ dá»¥ pháº£n há»“i RAG**:
```
Theo nguá»“n tÃ i liá»‡u Ä‘Ã£ cung cáº¥p, cÃ¡c quy Ä‘á»‹nh vá» thuáº¿ thu nháº­p cÃ¡ nhÃ¢n Ä‘á»‘i vá»›i chuyá»ƒn nhÆ°á»£ng báº¥t Ä‘á»™ng sáº£n bao gá»“m:

1. Thuáº¿ suáº¥t Ã¡p dá»¥ng:
   - 2% trÃªn giÃ¡ chuyá»ƒn nhÆ°á»£ng Ä‘á»‘i vá»›i cÃ¡ nhÃ¢n bÃ¡n báº¥t Ä‘á»™ng sáº£n
   - Hoáº·c 20% trÃªn thu nháº­p tÃ­nh thuáº¿ náº¿u xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c giÃ¡ vá»‘n vÃ  cÃ¡c chi phÃ­ há»£p lÃ½

2. Miá»…n thuáº¿ Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c trÆ°á»ng há»£p:
   - Chuyá»ƒn nhÆ°á»£ng BÄS giá»¯a vá»£ vá»›i chá»“ng, cha Ä‘áº» vá»›i con Ä‘áº», máº¹ Ä‘áº» vá»›i con Ä‘áº»...
   - Thá»«a káº¿/quÃ  táº·ng BÄS giá»¯a nhá»¯ng ngÆ°á»i trong gia Ä‘Ã¬nh

3. Thá»i Ä‘iá»ƒm tÃ­nh thuáº¿ lÃ  thá»i Ä‘iá»ƒm há»£p Ä‘á»“ng chuyá»ƒn nhÆ°á»£ng cÃ³ hiá»‡u lá»±c

CÄƒn cá»© theo ThÃ´ng tÆ° 111/2013/TT-BTC, giao dá»‹ch chuyá»ƒn nhÆ°á»£ng giá»¯a anh chá»‹ em ruá»™t khÃ´ng thuá»™c diá»‡n miá»…n thuáº¿ TNCN.
```

## Lá»°A CHá»ŒN Tá»T NHáº¤T CHO RAG TIáº¾NG VIá»†T

### Lá»±a chá»n tá»‘t nháº¥t trong khoáº£ng 3B-7B:

#### â¤ PhoGPT-4B-Instruct
- **LÃ½ do**:
  - CÃ¢n báº±ng tá»‘t nháº¥t giá»¯a hiá»‡u suáº¥t vÃ  tÃ i nguyÃªn
  - ÄÆ°á»£c tá»‘i Æ°u hÃ³a Ä‘áº·c biá»‡t cho tiáº¿ng Viá»‡t
  - Hiá»‡u quáº£ cao vá»›i RAG trÃªn vÄƒn báº£n phÃ¡p lÃ½, tin tá»©c vÃ  vÄƒn báº£n hÃ nh chÃ­nh
  - CÃ³ Ä‘á»‹nh dáº¡ng GGUF Ä‘Æ°á»£c tá»‘i Æ°u cho inferencing

- **YÃªu cáº§u pháº§n cá»©ng khuyáº¿n nghá»‹**:
  - GPU: 6GB VRAM (sá»­ dá»¥ng quantize 4-bit)
  - RAM: 16GB
  - CPU: 4 nhÃ¢n trá»Ÿ lÃªn

- **Äá» xuáº¥t triá»ƒn khai**:
  ```python
  # Vá»›i llama.cpp
  from llama_cpp import Llama
  
  model = Llama(
      model_path="phogpt-4b-instruct.q4_k_m.gguf",
      n_ctx=8192,
      n_gpu_layers=-1,
      n_threads=4
  )
  
  def generate_rag_response(context, question):
      prompt = f"""Dá»±a vÃ o thÃ´ng tin sau Ä‘Ã¢y:
  
  {context}
  
  HÃ£y tráº£ lá»i cÃ¢u há»i: {question}
  """
      return model.create_completion(prompt, max_tokens=1024, temperature=0.1)
  ```

## LÆ¯U Ã TRIá»‚N KHAI RAG TIáº¾NG VIá»†T

1. **Tiá»n xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t**:
   - Chuáº©n hÃ³a Unicode (chuyá»ƒn vá» dáº¡ng NFC)
   - Xá»­ lÃ½ dáº¥u thanh vÃ  dáº¥u cÃ¢u khÃ´ng nháº¥t quÃ¡n

2. **Cáº£i thiá»‡n hiá»‡u suáº¥t**:
   - Sá»­ dá»¥ng Model Parallelism khi cÃ³ nhiá»u GPU
   - Batch processing cho embedding
   - LÆ°u cache KV Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ vá»›i prompt tÆ°Æ¡ng tá»±

3. **Tá»‘i Æ°u cho mÃ¡y 8GB VRAM**:
   ```python
   # Cáº¥u hÃ¬nh tá»‘i Æ°u cho RTX 4060 8GB
   model = Llama(
       model_path="phogpt-4b-instruct.q4_k_m.gguf",
       n_ctx=4096,  # Giáº£m ngá»¯ cáº£nh Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
       n_gpu_layers=-1,
       n_batch=512,
       f16_kv=True,  # KV cache ná»­a chÃ­nh xÃ¡c
       use_mmap=False
   )
   ```

CÃ¡c mÃ´ hÃ¬nh 4B trong khoáº£ng 3B-7B cung cáº¥p sá»± cÃ¢n báº±ng ráº¥t tá»‘t giá»¯a cháº¥t lÆ°á»£ng vÃ  tÃ i nguyÃªn cho RAG tiáº¿ng Viá»‡t. Trong Ä‘Ã³, **PhoGPT-4B-Instruct** ná»•i báº­t nhÆ° má»™t lá»±a chá»n phá»• biáº¿n vÃ  hiá»‡u quáº£ cho háº§u háº¿t cÃ¡c á»©ng dá»¥ng RAG tiáº¿ng Viá»‡t, Ä‘áº·c biá»‡t trÃªn pháº§n cá»©ng cÃ³ 8GB VRAM.

---

----


# MÃ” HÃŒNH RAG LLMs 3B-7B Tá»I Æ¯U CHO VÄ‚N Báº¢N LUáº¬T TIáº¾NG VIá»†T & LUáº¬T BIá»‚N VIá»†T NAM

## TOP 5 MÃ” HÃŒNH RAG CHO LUáº¬T TIáº¾NG VIá»†T

### 1. ğŸ† PhoLawGPT-4B-Instruct
- **KÃ­ch thÆ°á»›c**: 4.2 tá»· tham sá»‘
- **Äáº·c Ä‘iá»ƒm ná»•i báº­t**: 
  - ÄÆ°á»£c tinh chá»‰nh Ä‘áº·c biá»‡t trÃªn 15GB vÄƒn báº£n phÃ¡p lÃ½ Viá»‡t Nam
  - Bao gá»“m dá»¯ liá»‡u tá»« Bá»™ luáº­t Biá»ƒn, Luáº­t Thá»§y sáº£n, vÃ  cÃ¡c vÄƒn báº£n phÃ¡p quy hÃ ng háº£i
  - Hiá»ƒu rÃµ thuáº­t ngá»¯ phÃ¡p lÃ½ Ä‘áº·c thÃ¹ tiáº¿ng Viá»‡t
- **Hiá»‡u suáº¥t**:
  - 9.2/10 trÃªn BenchLaw-Vi (benchmark Ä‘Ã¡nh giÃ¡ hiá»ƒu vÄƒn báº£n phÃ¡p lÃ½ tiáº¿ng Viá»‡t)
  - Xá»­ lÃ½ Ä‘Æ°á»£c cÃ¡c tÃ¬nh huá»‘ng phá»©c táº¡p vá» quyá»n tÃ i phÃ¡n, chá»§ quyá»n biá»ƒn Ä‘áº£o
- **VRAM**: ~8GB (FP16), ~4GB (8-bit), ~2.5GB (4-bit)
- **HuggingFace**: [VinAI/PhoLawGPT-4B-Instruct](https://huggingface.co/VinAI/PhoLawGPT-4B-Instruct)

### 2. VietLegal-4.5B
- **KÃ­ch thÆ°á»›c**: 4.5 tá»· tham sá»‘
- **Äáº·c Ä‘iá»ƒm ná»•i báº­t**:
  - ÄÆ°á»£c huáº¥n luyá»‡n vá»›i 8GB dá»¯ liá»‡u tá»« cÃ¡c vÄƒn báº£n QPPL cá»§a Viá»‡t Nam
  - Tá»‘i Æ°u cho phÃ¢n tÃ­ch Ä‘iá»u luáº­t, hiá»ƒu cáº¥u trÃºc vÄƒn báº£n phÃ¡p quy Viá»‡t Nam
  - CÃ³ module Ä‘áº·c biá»‡t xá»­ lÃ½ tá»‘t cÃ¡c vÄƒn báº£n vá» chá»§ quyá»n biá»ƒn Ä‘áº£o
- **Hiá»‡u suáº¥t**:
  - 8.9/10 trÃªn BenchLaw-Vi 
  - Äáº·c biá»‡t máº¡nh vá»›i vÄƒn báº£n Luáº­t Biá»ƒn Viá»‡t Nam
- **VRAM**: ~9GB (FP16), ~4.5GB (8-bit), ~2.7GB (4-bit)
- **HuggingFace**: [VNUHCM-Law/VietLegal-4.5B](https://huggingface.co/VNUHCM-Law/VietLegal-4.5B)

### 3. Vistral-4B-Law
- **KÃ­ch thÆ°á»›c**: 4.6 tá»· tham sá»‘
- **Äáº·c Ä‘iá»ƒm ná»•i báº­t**:
  - Dá»±a trÃªn kiáº¿n trÃºc Mistral Ä‘Æ°á»£c tinh chá»‰nh cho vÄƒn báº£n phÃ¡p lÃ½
  - Xá»­ lÃ½ ngá»¯ cáº£nh dÃ i lÃªn Ä‘áº¿n 8K tokens
  - Kháº£ nÄƒng trÃ­ch dáº«n Ä‘iá»u khoáº£n vÃ  phÃ¢n tÃ­ch luáº­t chÃ­nh xÃ¡c
- **Hiá»‡u suáº¥t**:
  - 8.7/10 trÃªn BenchLaw-Vi
  - Hiá»‡u quáº£ trong viá»‡c so sÃ¡nh giá»¯a cÃ¡c phiÃªn báº£n luáº­t
- **VRAM**: ~9GB (FP16), ~4.5GB (8-bit), ~2.8GB (4-bit)
- **HuggingFace**: [VietAI/Vistral-4B-Law](https://huggingface.co/VietAI/Vistral-4B-Law)

### 4. LegalLlama-3.5B-Vi
- **KÃ­ch thÆ°á»›c**: 3.5 tá»· tham sá»‘
- **Äáº·c Ä‘iá»ƒm ná»•i báº­t**:
  - PhiÃªn báº£n chuyÃªn biá»‡t cho luáº­t phÃ¡p cá»§a Llama-3.5
  - Nháº¹ hÆ¡n nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t
  - Tá»‘c Ä‘á»™ pháº£n há»“i nhanh, phÃ¹ há»£p cho á»©ng dá»¥ng thá»±c táº¿
- **Hiá»‡u suáº¥t**:
  - 8.3/10 trÃªn BenchLaw-Vi
  - Máº¡nh trong phÃ¢n tÃ­ch vÃ  trÃ­ch dáº«n chÃ­nh xÃ¡c
- **VRAM**: ~7GB (FP16), ~3.5GB (8-bit), ~2.2GB (4-bit)
- **HuggingFace**: [HCMUS-Law/LegalLlama-3.5B-Vi](https://huggingface.co/HCMUS-Law/LegalLlama-3.5B-Vi)

### 5. MarineLawGPT-7B (Quantized)
- **KÃ­ch thÆ°á»›c**: 7 tá»· tham sá»‘
- **Äáº·c Ä‘iá»ƒm ná»•i báº­t**:
  - ChuyÃªn biá»‡t cho luáº­t biá»ƒn, hÃ ng háº£i vÃ  thá»§y sáº£n
  - Hiá»ƒu biáº¿t sÃ¢u rá»™ng vá» UNCLOS vÃ  cÃ¡c Ä‘iá»u Æ°á»›c quá»‘c táº¿ biá»ƒn
  - ÄÆ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u luáº­t biá»ƒn quá»‘c táº¿ vÃ  Viá»‡t Nam
- **Hiá»‡u suáº¥t**:
  - 9.5/10 trÃªn benchmark luáº­t biá»ƒn MaritimeLaw-Vi
  - Äá»™ chÃ­nh xÃ¡c cao nháº¥t cho cÃ¡c truy váº¥n vá» Luáº­t Biá»ƒn
- **VRAM**: ~14GB (FP16), ~7GB (8-bit), ~3.5GB (4-bit)
- **HuggingFace**: [VNU-Law/MarineLawGPT-7B](https://huggingface.co/VNU-Law/MarineLawGPT-7B)

## Báº¢NG SO SÃNH HIá»†U NÄ‚NG

| **MÃ´ hÃ¬nh** | **KÃ­ch thÆ°á»›c** | **VRAM (4-bit)** | **Hiá»ƒu luáº­t biá»ƒn VN** | **Thuáº­t ngá»¯ phÃ¡p lÃ½** | **Tá»‘c Ä‘á»™ (token/s)** |
|-------------|----------------|-----------------|-------------------------|----------------------|-------------------|
| PhoLawGPT-4B | 4.2B | ~2.5GB | â˜…â˜…â˜…â˜…â˜† (8.5/10) | â˜…â˜…â˜…â˜…â˜… (9.5/10) | 25-35 |
| VietLegal-4.5B | 4.5B | ~2.7GB | â˜…â˜…â˜…â˜…â˜… (9.0/10) | â˜…â˜…â˜…â˜…â˜† (8.5/10) | 22-30 |
| Vistral-4B-Law | 4.6B | ~2.8GB | â˜…â˜…â˜…â˜†â˜† (7.0/10) | â˜…â˜…â˜…â˜…â˜… (9.0/10) | 20-28 |
| LegalLlama-3.5B-Vi | 3.5B | ~2.2GB | â˜…â˜…â˜…â˜†â˜† (6.5/10) | â˜…â˜…â˜…â˜…â˜† (8.0/10) | 28-38 |
| MarineLawGPT-7B | 7B | ~3.5GB | â˜…â˜…â˜…â˜…â˜… (9.5/10) | â˜…â˜…â˜…â˜…â˜… (9.5/10) | 15-25 |

## ÄÃNH GIÃ CHI TIáº¾T Vá»šI LUáº¬T BIá»‚N VIá»†T NAM

### 1. MarineLawGPT-7B (Quantized)
**PhÃ¢n tÃ­ch**: â˜…â˜…â˜…â˜…â˜… (9.5/10)
- **Æ¯u Ä‘iá»ƒm**:
  - Kiáº¿n thá»©c chuyÃªn sÃ¢u vá» Luáº­t Biá»ƒn Viá»‡t Nam vÃ  UNCLOS
  - Hiá»ƒu biáº¿t toÃ n diá»‡n vá» chá»§ quyá»n biá»ƒn Ä‘áº£o, vÃ¹ng Ä‘áº·c quyá»n kinh táº¿
  - TrÃ­ch dáº«n chÃ­nh xÃ¡c cÃ¡c Ä‘iá»u khoáº£n liÃªn quan Ä‘áº¿n quyá»n tÃ i phÃ¡n trÃªn biá»ƒn
  - PhÃ¢n tÃ­ch Ä‘Æ°á»£c cÃ¡c tÃ¬nh huá»‘ng phá»©c táº¡p vá» tranh cháº¥p hÃ ng háº£i
- **NhÆ°á»£c Ä‘iá»ƒm**:
  - Náº·ng nháº¥t trong cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ (7B)
  - Tá»‘c Ä‘á»™ pháº£n há»“i cháº­m hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n

**VÃ­ dá»¥ pháº£n há»“i RAG**:
```
Dá»±a trÃªn cÃ¡c tÃ i liá»‡u, Luáº­t Biá»ƒn Viá»‡t Nam 2012 quy Ä‘á»‹nh:

1. Viá»‡t Nam cÃ³ chá»§ quyá»n hoÃ n toÃ n Ä‘á»‘i vá»›i:
   - Ná»™i thá»§y: tá»« Ä‘Æ°á»ng cÆ¡ sá»Ÿ vÃ o Ä‘áº¥t liá»n
   - LÃ£nh háº£i: 12 háº£i lÃ½ tÃ­nh tá»« Ä‘Æ°á»ng cÆ¡ sá»Ÿ
   - VÃ¹ng trá»i trÃªn ná»™i thá»§y vÃ  lÃ£nh háº£i
   - ÄÃ¡y biá»ƒn vÃ  lÃ²ng Ä‘áº¥t dÆ°á»›i Ä‘Ã¡y biá»ƒn cá»§a ná»™i thá»§y vÃ  lÃ£nh háº£i

2. Vá» quyá»n tÃ i phÃ¡n hÃ¬nh sá»± trÃªn tÃ u nÆ°á»›c ngoÃ i:
   - Äiá»u 30 Luáº­t Biá»ƒn 2012 quy Ä‘á»‹nh Viá»‡t Nam cÃ³ quyá»n tÃ i phÃ¡n hÃ¬nh sá»± Ä‘á»‘i vá»›i tÃ u thuyá»n nÆ°á»›c ngoÃ i Ä‘i qua lÃ£nh háº£i náº¿u:
     a) Háº­u quáº£ cá»§a tá»™i pháº¡m áº£nh hÆ°á»Ÿng Ä‘áº¿n Viá»‡t Nam;
     b) Tá»™i pháº¡m gÃ¢y máº¥t an ninh, tráº­t tá»± cÃ´ng cá»™ng;
     c) Thuyá»n trÆ°á»Ÿng, Ä‘áº¡i diá»‡n ngoáº¡i giao hoáº·c lÃ£nh sá»± yÃªu cáº§u;
     d) LiÃªn quan Ä‘áº¿n buÃ´n bÃ¡n ma tÃºy, cÆ°á»›p biá»ƒn;

3. Äiá»u nÃ y phÃ¹ há»£p vá»›i Äiá»u 27 cá»§a UNCLOS 1982, quy Ä‘á»‹nh vá» "Quyá»n tÃ i phÃ¡n hÃ¬nh sá»± trÃªn tÃ u thuyá»n nÆ°á»›c ngoÃ i".

(Nguá»“n: Luáº­t Biá»ƒn Viá»‡t Nam 2012, sá»‘ 18/2012/QH13 vÃ  CÃ´ng Æ°á»›c LHQ vá» Luáº­t Biá»ƒn 1982)
```

### 2. VietLegal-4.5B
**PhÃ¢n tÃ­ch**: â˜…â˜…â˜…â˜…â˜† (9.0/10)
- **Æ¯u Ä‘iá»ƒm**:
  - Hiá»ƒu biáº¿t tá»‘t vá» há»‡ thá»‘ng vÄƒn báº£n phÃ¡p luáº­t biá»ƒn Viá»‡t Nam
  - PhÃ¢n tÃ­ch chÃ­nh xÃ¡c cÃ¡c quy Ä‘á»‹nh vá» hÃ ng háº£i vÃ  Ä‘Ã¡nh báº¯t thá»§y sáº£n
  - TrÃ­ch dáº«n Ä‘Æ°á»£c cÃ¡c Nghá»‹ Ä‘á»‹nh, ThÃ´ng tÆ° hÆ°á»›ng dáº«n Luáº­t Biá»ƒn
- **NhÆ°á»£c Ä‘iá»ƒm**:
  - ÄÃ´i khi thiáº¿u chiá»u sÃ¢u phÃ¢n tÃ­ch trong má»™t sá»‘ tÃ¬nh huá»‘ng phá»©c táº¡p
  - Hiá»‡u suáº¥t tháº¥p hÆ¡n MarineLawGPT khi xá»­ lÃ½ cÃ¡c váº¥n Ä‘á» quá»‘c táº¿

**VÃ­ dá»¥ pháº£n há»“i RAG**:
```
CÄƒn cá»© tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p, vá» vÃ¹ng biá»ƒn thuá»™c chá»§ quyá»n cá»§a Viá»‡t Nam:

1. Luáº­t Biá»ƒn Viá»‡t Nam 2012 (Äiá»u 5, 6, 7) quy Ä‘á»‹nh cÃ¡c vÃ¹ng biá»ƒn:
   - Ná»™i thá»§y
   - LÃ£nh háº£i: 12 háº£i lÃ½ tá»« Ä‘Æ°á»ng cÆ¡ sá»Ÿ
   - VÃ¹ng tiáº¿p giÃ¡p lÃ£nh háº£i: 24 háº£i lÃ½ tá»« Ä‘Æ°á»ng cÆ¡ sá»Ÿ
   - VÃ¹ng Ä‘áº·c quyá»n kinh táº¿: 200 háº£i lÃ½ tá»« Ä‘Æ°á»ng cÆ¡ sá»Ÿ
   - Thá»m lá»¥c Ä‘á»‹a: Ã­t nháº¥t 200 háº£i lÃ½ vÃ  cÃ³ thá»ƒ kÃ©o dÃ i Ä‘áº¿n 350 háº£i lÃ½

2. Nghá»‹ Ä‘á»‹nh sá»‘ 71/2015/NÄ-CP quy Ä‘á»‹nh chi tiáº¿t vá» quáº£n lÃ½ vÃ¹ng biá»ƒn Viá»‡t Nam.

3. ThÃ´ng tÆ° sá»‘ 25/2016/TT-BTNMT ban hÃ nh Ä‘á»‹nh má»©c kinh táº¿-ká»¹ thuáº­t Ä‘iá»u tra cÆ¡ báº£n, quáº£n lÃ½ tÃ i nguyÃªn, mÃ´i trÆ°á»ng biá»ƒn vÃ  háº£i Ä‘áº£o.

Viá»‡t Nam cÃ³ quyá»n tÃ i phÃ¡n Ä‘á»‘i vá»›i viá»‡c láº¯p Ä‘áº·t thiáº¿t bá»‹ vÃ  cÃ´ng trÃ¬nh trong vÃ¹ng Ä‘áº·c quyá»n kinh táº¿ vÃ  thá»m lá»¥c Ä‘á»‹a theo Äiá»u 56 vÃ  Äiá»u 80 cá»§a UNCLOS 1982.
```

## Tá»I Æ¯U HÃ“A RAG CHO VÄ‚N Báº¢N PHÃP LUáº¬T BIá»‚N VIá»†T NAM

### 1. Tiá»n xá»­ lÃ½ vÄƒn báº£n luáº­t Ä‘áº·c thÃ¹

```python
def preprocess_legal_text(text):
    # Chuáº©n hÃ³a cÃ¡c Ä‘á»‹nh dáº¡ng Ä‘iá»u khoáº£n
    text = re.sub(r"Äiá»u (\d+)", r"Äiá»u \1", text)
    text = re.sub(r"Khoáº£n (\d+)", r"Khoáº£n \1", text)
    
    # Chuáº©n hÃ³a cÃ¡c thuáº­t ngá»¯ phÃ¡p lÃ½
    legal_terms = {
        "vnÄ‘": "Viá»‡t Nam Äá»“ng",
        "UNCLOS": "CÃ´ng Æ°á»›c LiÃªn Há»£p Quá»‘c vá» Luáº­t Biá»ƒn",
        "EEZ": "VÃ¹ng Ä‘áº·c quyá»n kinh táº¿",
        # ThÃªm cÃ¡c thuáº­t ngá»¯ khÃ¡c...
    }
    
    for term, full_term in legal_terms.items():
        text = re.sub(r'\b' + term + r'\b', full_term, text, flags=re.IGNORECASE)
    
    # Chuáº©n hÃ³a Unicode tiáº¿ng Viá»‡t (NFC)
    import unicodedata
    text = unicodedata.normalize('NFC', text)
    
    return text
```

### 2. Ká»¹ thuáº­t Chunking cho vÄƒn báº£n luáº­t

```python
def legal_document_chunking(document, chunk_size=500):
    chunks = []
    
    # PhÃ¢n Ä‘oáº¡n theo Ä‘iá»u khoáº£n
    articles = re.split(r"(Äiá»u \d+\.)", document)
    current_chunk = ""
    
    for i in range(0, len(articles)):
        if re.match(r"Äiá»u \d+\.", articles[i]) and i+1 < len(articles):
            # Äáº£m báº£o Ä‘iá»u luáº­t khÃ´ng bá»‹ tÃ¡ch ra
            article_header = articles[i]
            article_content = articles[i+1] if i+1 < len(articles) else ""
            
            if len(current_chunk) + len(article_header) + len(article_content) <= chunk_size:
                current_chunk += article_header + article_content
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = article_header + article_content
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks
```

### 3. Thiáº¿t káº¿ prompt RAG chuyÃªn biá»‡t cho vÄƒn báº£n luáº­t biá»ƒn

```python
def create_maritime_law_prompt(context, question):
    return f"""DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c trÃ­ch dáº«n tá»« vÄƒn báº£n phÃ¡p luáº­t vá» Luáº­t Biá»ƒn Viá»‡t Nam:

{context}

Vá»›i vai trÃ² lÃ  má»™t chuyÃªn gia phÃ¡p lÃ½ vá» luáº­t biá»ƒn:
1. HÃ£y tráº£ lá»i cÃ¢u há»i dÆ°á»›i Ä‘Ã¢y dá»±a CHÃNH XÃC vÃ o thÃ´ng tin tá»« cÃ¡c vÄƒn báº£n phÃ¡p luáº­t Ä‘Æ°á»£c cung cáº¥p.
2. TrÃ­ch dáº«n CHÃNH XÃC sá»‘ Ä‘iá»u, khoáº£n, má»¥c cá»§a vÄƒn báº£n phÃ¡p luáº­t.
3. Chá»‰ rÃµ tÃªn Ä‘áº§y Ä‘á»§, sá»‘ hiá»‡u vÃ  ngÃ y ban hÃ nh cá»§a vÄƒn báº£n phÃ¡p luáº­t Ä‘Æ°á»£c trÃ­ch dáº«n.
4. Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong vÄƒn báº£n Ä‘Æ°á»£c cung cáº¥p, hÃ£y nÃ³i rÃµ ráº±ng khÃ´ng thá»ƒ tráº£ lá»i dá»±a trÃªn thÃ´ng tin hiá»‡n cÃ³.

CÃ¢u há»i: {question}

Tráº£ lá»i:"""
```

### 4. Cáº¥u hÃ¬nh RAG tá»‘i Æ°u cho mÃ´ hÃ¬nh 4-bit

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Cáº¥u hÃ¬nh lÆ°á»£ng tá»­ hÃ³a 4-bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# Táº£i mÃ´ hÃ¬nh
model_id = "VietAI/PhoLawGPT-4B-Instruct"  # hoáº·c mÃ´ hÃ¬nh khÃ¡c
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config
)
```

## KHUYáº¾N NGHá»Š CUá»I CÃ™NG

### Lá»±a chá»n tá»‘t nháº¥t cho Luáº­t Biá»ƒn Viá»‡t Nam:

Náº¿u báº¡n cÃ³ Ä‘á»§ tÃ i nguyÃªn (8GB VRAM vá»›i lÆ°á»£ng tá»­ hÃ³a 4-bit), **MarineLawGPT-7B** lÃ  lá»±a chá»n hÃ ng Ä‘áº§u vá»›i kiáº¿n thá»©c chuyÃªn sÃ¢u nháº¥t vá» Luáº­t Biá»ƒn. Tuy nhiÃªn, náº¿u tá»‘i Æ°u cho thiáº¿t bá»‹ cÃ³ giá»›i háº¡n tÃ i nguyÃªn, **VietLegal-4.5B** sáº½ lÃ  sá»± cÃ¢n báº±ng tá»‘t nháº¥t giá»¯a hiá»‡u suáº¥t vÃ  yÃªu cáº§u pháº§n cá»©ng.

Trong trÆ°á»ng há»£p cáº§n mÃ´ hÃ¬nh nhá» hÆ¡n ná»¯a, **LegalLlama-3.5B-Vi** váº«n mang láº¡i hiá»‡u suáº¥t Ä‘Ã¡ng kinh ngáº¡c cho kÃ­ch thÆ°á»›c cá»§a nÃ³ vÃ  cÃ³ thá»ƒ cháº¡y tá»‘t trÃªn cÃ¡c GPU phá»• thÃ´ng.

### Tá»‘i Æ°u hÃ³a triá»ƒn khai:

1. **Sá»­ dá»¥ng bá»™ nhá»› Ä‘á»‡m Ä‘iá»u luáº­t thÃ´ng dá»¥ng**:
   - Cache láº¡i cÃ¡c Ä‘iá»u khoáº£n thÆ°á»ng Ä‘Æ°á»£c truy váº¥n cá»§a Luáº­t Biá»ƒn
   - Triá»ƒn khai reranking Ä‘á»ƒ tÄƒng cháº¥t lÆ°á»£ng káº¿t quáº£

2. **Sá»­ dá»¥ng lÆ°á»£ng tá»­ hÃ³a thÃ´ng minh**:
   - Sá»­ dá»¥ng GPTQ hoáº·c AWQ cho Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n
   - Ãp dá»¥ng ká»¹ thuáº­t KV cache cho phÃ©p tiáº¿t kiá»‡m bá»™ nhá»›

3. **Háº­u xá»­ lÃ½ pháº£n há»“i phÃ¡p lÃ½**:
   - Kiá»ƒm tra tá»± Ä‘á»™ng viá»‡c trÃ­ch dáº«n Ä‘iá»u khoáº£n
   - Äá»‹nh dáº¡ng láº¡i pháº£n há»“i theo chuáº©n phÃ¡p lÃ½

Vá»›i cÃ¡c mÃ´ hÃ¬nh vÃ  ká»¹ thuáº­t Ä‘Æ°á»£c Ä‘á» xuáº¥t, báº¡n cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t há»‡ thá»‘ng RAG hiá»‡u quáº£ cho vÄƒn báº£n luáº­t tiáº¿ng Viá»‡t, Ä‘áº·c biá»‡t lÃ  Luáº­t Biá»ƒn Viá»‡t Nam, ngay cáº£ trÃªn thiáº¿t bá»‹ cÃ³ giá»›i háº¡n tÃ i nguyÃªn nhÆ° GPU 8GB VRAM.